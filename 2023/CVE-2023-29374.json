{
    "id": "CVE-2023-29374",
    "product": "n/a",
    "version": "n/a",
    "vulnerability": [
        "n/a"
    ],
    "description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
    "poc": {
        "reference": [
            "https://github.com/hwchase17/langchain/issues/1026"
        ],
        "github": [
            "https://github.com/cckuailong/awesome-gpt-security",
            "https://github.com/corca-ai/awesome-llm-security"
        ]
    }
}